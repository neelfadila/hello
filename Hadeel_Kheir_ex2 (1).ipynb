{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Libraries (don't change)"
   ],
   "metadata": {
    "id": "Q3S5loqBCqgM"
   },
   "id": "Q3S5loqBCqgM"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ba5ec",
   "metadata": {
    "id": "5f1ba5ec"
   },
   "outputs": [],
   "source": [
    "!pip -q install torchinfo\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Callable, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Device (don't change)"
   ],
   "metadata": {
    "id": "ykrfYnc6g1Bs"
   },
   "id": "ykrfYnc6g1Bs"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "use_amp = (DEVICE == \"cuda\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Mixed precision (AMP): {use_amp}\")"
   ],
   "metadata": {
    "id": "XJIYaWIggSCp"
   },
   "id": "XJIYaWIggSCp",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d71ffe61",
   "metadata": {
    "id": "d71ffe61"
   },
   "source": [
    "Data (don't change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0bf0c",
   "metadata": {
    "id": "2cb0bf0c"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DataManager:\n",
    "    def __init__(self, dataset_class, root: str = \"./data\", val_fraction: float = 0.1,\n",
    "                 batch_size: int = 32, seed: int = 42):\n",
    "        self.dataset_class = dataset_class\n",
    "        self.root = root\n",
    "        self.val_fraction = val_fraction\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1918,), (0.3483,))\n",
    "        ])\n",
    "\n",
    "    def get_loaders(self) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        full_train = self.dataset_class(root=self.root, train=True,\n",
    "                                        download=True, transform=self.transform)\n",
    "        test_ds = self.dataset_class(root=self.root, train=False,\n",
    "                                     download=True, transform=self.transform)\n",
    "\n",
    "        val_size = int(len(full_train) * self.val_fraction)\n",
    "        train_size = len(full_train) - val_size\n",
    "\n",
    "        generator = torch.Generator().manual_seed(self.seed)\n",
    "        train_ds, val_ds = random_split(full_train, [train_size, val_size], generator=generator)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=self.batch_size,\n",
    "                                  shuffle=True, num_workers=2, pin_memory=True)\n",
    "        val_loader   = DataLoader(val_ds,   batch_size=self.batch_size,\n",
    "                                  shuffle=False, num_workers=2, pin_memory=True)\n",
    "        test_loader  = DataLoader(test_ds,  batch_size=self.batch_size,\n",
    "                                  shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "        print(f\"Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\")\n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Configurations (don't change)"
   ],
   "metadata": {
    "id": "O0SlvxE36HG0"
   },
   "id": "O0SlvxE36HG0"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "@dataclass\n",
    "class LayerSpec:\n",
    "    out_dim: int\n",
    "    activation: Callable[[torch.Tensor], torch.Tensor] = F.relu\n",
    "    dropout: float = 0.0\n",
    "    batch_norm: bool = True\n",
    "    weight_decay: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    input_dim: Tuple[int, int, int] = (1, 28, 28)\n",
    "    num_classes: int = 10\n",
    "    layers: List[LayerSpec] = None\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    batch_size: int = 64\n",
    "    epochs: int = 100\n",
    "    lr: float = 1e-4\n",
    "    patience: int = 15\n",
    "    min_delta: float = 1e-4\n",
    "    val_fraction: float = 0.1\n",
    "    seed: int = 42\n"
   ],
   "metadata": {
    "id": "FwDGf9J66bXL"
   },
   "id": "FwDGf9J66bXL",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "53221445",
   "metadata": {
    "id": "53221445"
   },
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5843387",
   "metadata": {
    "id": "b5843387"
   },
   "outputs": [],
   "source": [
    "    \n",
    "class MLPFromConfig(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        flat_dim = config.input_dim[0] * config.input_dim[1] * config.input_dim[2]\n",
    "        self.layers_specs = config.layers\n",
    "        layers = []\n",
    "        prev_dim = flat_dim\n",
    "\n",
    "        for i, spec in enumerate(config.layers):\n",
    "            linear = nn.Linear(prev_dim, spec.out_dim)\n",
    "\n",
    "            layers.append(linear)\n",
    "            if spec.batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(spec.out_dim))\n",
    "            if spec.dropout > 0:\n",
    "                layers.append(nn.Dropout(spec.dropout))\n",
    "            layers.append(spec.activation())\n",
    "            prev_dim = spec.out_dim\n",
    "\n",
    "        # Final classifier layer\n",
    "        self.final_linear = nn.Linear(prev_dim, config.num_classes)\n",
    "        layers.append(self.final_linear)\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.net(x)\n",
    "\n",
    "    def get_layer_params(self):\n",
    "        param_groups = []\n",
    "        for i, spec in enumerate(self.layers_specs):\n",
    "            linear_layer = self.net[i * (4 if spec.batch_norm or spec.dropout > 0 else 3)]\n",
    "            pass\n",
    "        return self.layers_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Early Stopping (don't change)"
   ],
   "metadata": {
    "id": "eND74vML5XYh"
   },
   "id": "eND74vML5XYh"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 10, min_delta: float = 1e-4):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.should_stop = False\n",
    "\n",
    "    def __call__(self, val_loss: float) -> bool:\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "        return self.should_stop"
   ],
   "metadata": {
    "id": "KgImYRwI5hAr"
   },
   "id": "KgImYRwI5hAr",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9b11a04f",
   "metadata": {
    "id": "9b11a04f"
   },
   "source": [
    "Trainer (don't change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e474713f",
   "metadata": {
    "id": "e474713f"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module, config: TrainConfig):\n",
    "        self.model = model.to(DEVICE)\n",
    "        self.config = config\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = self._build_optimizer()\n",
    "        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "        self.early_stopping = EarlyStopping(patience=config.patience,\n",
    "                                            min_delta=config.min_delta)\n",
    "\n",
    "        self.history = {\"train_loss\": [], \"train_acc\": [],\n",
    "                        \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "\n",
    "        # Collect all Linear layers in the order they appear\n",
    "        linear_layers = []\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                linear_layers.append((name, module))\n",
    "\n",
    "        param_groups = []\n",
    "\n",
    "        for i, spec in enumerate(self.model.layers_specs):\n",
    "            name, layer = linear_layers[i]\n",
    "            param_groups.append({\n",
    "                'params': layer.parameters(),\n",
    "                'weight_decay': spec.weight_decay\n",
    "            })\n",
    "\n",
    "        final_name, final_layer = linear_layers[-1]\n",
    "        param_groups.append({\n",
    "            'params': final_layer.parameters(),\n",
    "            'weight_decay': 0.0\n",
    "        })\n",
    "\n",
    "        return torch.optim.SGD(param_groups, momentum=0.9, nesterov=True, lr=self.config.lr)\n",
    "\n",
    "    def _train_epoch(self, loader: DataLoader):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            correct += (output.argmax(1) == target).sum().item()\n",
    "            total += data.size(0)\n",
    "\n",
    "        return total_loss / total, correct / total\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _eval_epoch(self, loader: DataLoader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            correct += (output.argmax(1) == target).sum().item()\n",
    "            total += data.size(0)\n",
    "\n",
    "        return total_loss / total, correct / total\n",
    "\n",
    "    def fit(self, train_loader: DataLoader, val_loader: DataLoader):\n",
    "        print(\"üöÄ Starting training...\\n\")\n",
    "        for epoch in range(1, self.config.epochs + 1):\n",
    "            train_loss, train_acc = self._train_epoch(train_loader)\n",
    "            val_loss, val_acc     = self._eval_epoch(val_loader)\n",
    "\n",
    "            self.history[\"train_loss\"].append(train_loss)\n",
    "            self.history[\"train_acc\"].append(train_acc)\n",
    "            self.history[\"val_loss\"].append(val_loss)\n",
    "            self.history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "            print(f\"Epoch {epoch:3d} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "            if self.early_stopping(val_loss):\n",
    "                print(f\"\\nüõë Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        print(\"\\n‚úÖ Training complete!\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, loader: DataLoader):\n",
    "        return self._eval_epoch(loader)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_all(self, loader: DataLoader):\n",
    "        self.model.eval()\n",
    "        all_preds, all_targets = [], []\n",
    "        for x, y in loader:\n",
    "            x = x.to(DEVICE, non_blocking=True)\n",
    "            logits = self.model(x)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_targets.append(y.numpy())\n",
    "        return np.concatenate(all_preds), np.concatenate(all_targets)\n",
    "\n",
    "\n",
    "    def save(self, path: str = \"mlp_best.pt\"):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        print(f\"üíæ Model saved to {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7d02d",
   "metadata": {
    "id": "55c7d02d"
   },
   "source": [
    "\n",
    "Run (do change)"
   ]
  },
  {
   "cell_type": "code",
   "id": "d133bb1a",
   "metadata": {
    "id": "d133bb1a",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "train_cfg = TrainConfig(\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    "    lr=1e-4,\n",
    "    patience=5,\n",
    "    val_fraction=0.1\n",
    ")\n",
    "\n",
    "data_mgr = DataManager(\n",
    "    dataset_class=datasets.KMNIST,\n",
    "    val_fraction=train_cfg.val_fraction,\n",
    "    batch_size=train_cfg.batch_size,\n",
    "    seed=train_cfg.seed\n",
    ")\n",
    "train_loader, val_loader, test_loader = data_mgr.get_loaders()\n",
    "\n",
    "model_cfg = ModelConfig(\n",
    "    layers=[\n",
    "        # Add layers here in the format of LayerSpec. For example\n",
    "        LayerSpec(out_dim=10,  dropout=0.1, batch_norm=False, activation = nn.ReLU, weight_decay=5e-1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = MLPFromConfig(model_cfg)\n",
    "print(summary(model, input_size=(1, 28, 28)))\n",
    "\n",
    "trainer = Trainer(model, train_cfg)\n",
    "trainer.fit(train_loader, val_loader)\n",
    "trainer.save(\"mlp_colab_best.pt\")\n",
    "\n",
    "\n",
    "try:\n",
    "    import mlflow\n",
    "except ImportError:\n",
    "    !pip -q install mlflow\n",
    "    import mlflow\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    rep = classification_report(y_true, y_pred, output_dict=True, digits=4, zero_division=0)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    out = {\"accuracy\": float(acc)}\n",
    "\n",
    "    # Per-class metrics (classes 0..9)\n",
    "    for c in range(10):\n",
    "        out[f\"precision_c{c}\"] = float(rep[str(c)][\"precision\"])\n",
    "        out[f\"recall_c{c}\"]    = float(rep[str(c)][\"recall\"])\n",
    "        out[f\"f1_c{c}\"]        = float(rep[str(c)][\"f1-score\"])\n",
    "\n",
    "    # Averages\n",
    "    out[\"precision_macro\"] = float(rep[\"macro avg\"][\"precision\"])\n",
    "    out[\"recall_macro\"]    = float(rep[\"macro avg\"][\"recall\"])\n",
    "    out[\"f1_macro\"]        = float(rep[\"macro avg\"][\"f1-score\"])\n",
    "\n",
    "    out[\"precision_weighted\"] = float(rep[\"weighted avg\"][\"precision\"])\n",
    "    out[\"recall_weighted\"]    = float(rep[\"weighted avg\"][\"recall\"])\n",
    "    out[\"f1_weighted\"]        = float(rep[\"weighted avg\"][\"f1-score\"])\n",
    "\n",
    "    return out\n",
    "\n",
    "mlflow.set_experiment(\"KMNIST_ex2\")\n",
    "\n",
    "\n",
    "experiments = [\n",
    "    (\"exp01_256_128_lr1e3\", 1e-3, [\n",
    "        LayerSpec(out_dim=256, dropout=0.1, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=128, dropout=0.1, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=10,  dropout=0.0, batch_norm=False, activation=nn.ReLU, weight_decay=0.0),\n",
    "    ]),\n",
    "    (\"exp02_512_256_lr1e3\", 1e-3, [\n",
    "        LayerSpec(out_dim=512, dropout=0.2, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=256, dropout=0.2, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=10,  dropout=0.0, batch_norm=False, activation=nn.ReLU, weight_decay=0.0),\n",
    "    ]),\n",
    "    (\"exp03_512_256_128_lr1e3\", 1e-3, [\n",
    "        LayerSpec(out_dim=512, dropout=0.3, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=256, dropout=0.3, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=128, dropout=0.2, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=10,  dropout=0.0, batch_norm=False, activation=nn.ReLU, weight_decay=0.0),\n",
    "    ]),\n",
    "    (\"exp04_256_128_noBN_lr1e3\", 1e-3, [\n",
    "        LayerSpec(out_dim=256, dropout=0.1, batch_norm=False, activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=128, dropout=0.1, batch_norm=False, activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=10,  dropout=0.0, batch_norm=False, activation=nn.ReLU, weight_decay=0.0),\n",
    "    ]),\n",
    "    (\"exp05_512_256_lr3e4\", 3e-4, [\n",
    "        LayerSpec(out_dim=512, dropout=0.2, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=256, dropout=0.2, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=10,  dropout=0.0, batch_norm=False, activation=nn.ReLU, weight_decay=0.0),\n",
    "    ]),\n",
    "    (\"exp06_512_256_lr2e3\", 2e-3, [\n",
    "        LayerSpec(out_dim=512, dropout=0.2, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=256, dropout=0.2, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=10,  dropout=0.0, batch_norm=False, activation=nn.ReLU, weight_decay=0.0),\n",
    "    ]),\n",
    "    (\"exp07_1024_256_lr1e3\", 1e-3, [\n",
    "        LayerSpec(out_dim=1024, dropout=0.3, batch_norm=True, activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=256,  dropout=0.3, batch_norm=True, activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=10,   dropout=0.0, batch_norm=False, activation=nn.ReLU, weight_decay=0.0),\n",
    "    ]),\n",
    "    (\"exp08_256_256_256_lr1e3\", 1e-3, [\n",
    "        LayerSpec(out_dim=256, dropout=0.2, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=256, dropout=0.2, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=256, dropout=0.2, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=10,  dropout=0.0, batch_norm=False, activation=nn.ReLU, weight_decay=0.0),\n",
    "    ]),\n",
    "    (\"exp09_512_512_lr1e3\", 1e-3, [\n",
    "        LayerSpec(out_dim=512, dropout=0.4, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=512, dropout=0.4, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-4),\n",
    "        LayerSpec(out_dim=10,  dropout=0.0, batch_norm=False, activation=nn.ReLU, weight_decay=0.0),\n",
    "    ]),\n",
    "    (\"exp10_512_256_wd1e3_lr1e3\", 1e-3, [\n",
    "        LayerSpec(out_dim=512, dropout=0.2, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-3),\n",
    "        LayerSpec(out_dim=256, dropout=0.2, batch_norm=True,  activation=nn.ReLU, weight_decay=1e-3),\n",
    "        LayerSpec(out_dim=10,  dropout=0.0, batch_norm=False, activation=nn.ReLU, weight_decay=0.0),\n",
    "    ]),\n",
    "]\n",
    "\n",
    "results_rows = []\n",
    "best_test_acc = -1.0\n",
    "best_model_path = \"mlp_colab_best.pt\"\n",
    "\n",
    "for exp_name, lr, layers in experiments:\n",
    "    # create per-experiment train config (we do NOT change your original train_cfg lines)\n",
    "    train_cfg_exp = TrainConfig(\n",
    "        batch_size=train_cfg.batch_size,\n",
    "        epochs=train_cfg.epochs,\n",
    "        lr=lr,\n",
    "        patience=train_cfg.patience,\n",
    "        val_fraction=train_cfg.val_fraction,\n",
    "        seed=train_cfg.seed\n",
    "    )\n",
    "\n",
    "    model_cfg_exp = ModelConfig(layers=layers)\n",
    "\n",
    "    with mlflow.start_run(run_name=exp_name) as run:\n",
    "        # log params\n",
    "        mlflow.log_param(\"exp_name\", exp_name)\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "        mlflow.log_param(\"epochs\", train_cfg_exp.epochs)\n",
    "        mlflow.log_param(\"batch_size\", train_cfg_exp.batch_size)\n",
    "        mlflow.log_param(\"patience\", train_cfg_exp.patience)\n",
    "        mlflow.log_param(\"val_fraction\", train_cfg_exp.val_fraction)\n",
    "\n",
    "        # build + train\n",
    "        model_exp = MLPFromConfig(model_cfg_exp)\n",
    "        trainer_exp = Trainer(model_exp, train_cfg_exp)\n",
    "        trainer_exp.fit(train_loader, val_loader)\n",
    "\n",
    "        # evaluate TRAIN + TEST (required)\n",
    "        preds_tr, targets_tr = trainer_exp.predict_all(train_loader)\n",
    "        preds_te, targets_te = trainer_exp.predict_all(test_loader)\n",
    "\n",
    "        train_metrics = compute_metrics(targets_tr, preds_tr)\n",
    "        test_metrics  = compute_metrics(targets_te, preds_te)\n",
    "\n",
    "        for k, v in train_metrics.items():\n",
    "            mlflow.log_metric(f\"train_{k}\", float(v))\n",
    "        for k, v in test_metrics.items():\n",
    "            mlflow.log_metric(f\"test_{k}\", float(v))\n",
    "\n",
    "        # keep best\n",
    "        if test_metrics[\"accuracy\"] > best_test_acc:\n",
    "            best_test_acc = test_metrics[\"accuracy\"]\n",
    "            trainer_exp.save(best_model_path)\n",
    "\n",
    "        # row for Excel\n",
    "        row = {\n",
    "            \"exp_name\": exp_name,\n",
    "            \"run_id\": run.info.run_id,\n",
    "            \"lr\": lr,\n",
    "            \"train_accuracy\": train_metrics[\"accuracy\"],\n",
    "            \"test_accuracy\": test_metrics[\"accuracy\"],\n",
    "            \"train_precision_macro\": train_metrics[\"precision_macro\"],\n",
    "            \"train_recall_macro\": train_metrics[\"recall_macro\"],\n",
    "            \"train_f1_macro\": train_metrics[\"f1_macro\"],\n",
    "            \"test_precision_macro\": test_metrics[\"precision_macro\"],\n",
    "            \"test_recall_macro\": test_metrics[\"recall_macro\"],\n",
    "            \"test_f1_macro\": test_metrics[\"f1_macro\"],\n",
    "        }\n",
    "        for c in range(10):\n",
    "            row[f\"train_precision_c{c}\"] = train_metrics[f\"precision_c{c}\"]\n",
    "            row[f\"train_recall_c{c}\"]    = train_metrics[f\"recall_c{c}\"]\n",
    "            row[f\"train_f1_c{c}\"]        = train_metrics[f\"f1_c{c}\"]\n",
    "            row[f\"test_precision_c{c}\"]  = test_metrics[f\"precision_c{c}\"]\n",
    "            row[f\"test_recall_c{c}\"]     = test_metrics[f\"recall_c{c}\"]\n",
    "            row[f\"test_f1_c{c}\"]         = test_metrics[f\"f1_c{c}\"]\n",
    "\n",
    "        results_rows.append(row)\n",
    "\n",
    "print(f\"‚úÖ Best TEST accuracy: {best_test_acc:.4f}\")\n",
    "print(f\"üíæ Best model saved as: {best_model_path}\")\n",
    "\n",
    "# Excel output\n",
    "df = pd.DataFrame(results_rows).sort_values(by=\"test_accuracy\", ascending=False)\n",
    "excel_path = \"ex2_results.xlsx\"\n",
    "df.to_excel(excel_path, index=False)\n",
    "print(f\"üìÑ Excel saved: {excel_path}\")\n",
    "\n",
    "df.head(10)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3e0c6f58",
   "metadata": {
    "id": "3e0c6f58"
   },
   "source": [
    "Visuazize the train\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "740b2e48",
   "metadata": {
    "id": "740b2e48",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "\n",
    "history = trainer.history\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"val_loss\"],   label=\"Val Loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history[\"train_acc\"], label=\"Train Acc\")\n",
    "plt.plot(history[\"val_acc\"],   label=\"Val Acc\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualized the test (do change)"
   ],
   "metadata": {
    "id": "5kjoDILMt2Zw"
   },
   "id": "5kjoDILMt2Zw"
  },
  {
   "cell_type": "code",
   "source": [
    "# entire test result\n",
    "test_loss, test_acc = trainer.evaluate(test_loader)\n",
    "print(f\"üèÜ Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# predictions\n",
    "preds, targets = trainer.predict_all(test_loader)\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(targets, preds)\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.title(\"Confusion Matrix (Test)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# per-class report\n",
    "print(\"Classification report (Test):\")\n",
    "print(classification_report(targets, preds, digits=4))\n",
    "\n",
    "# ===== TRAIN (required for overfitting check) =====\n",
    "train_loss, train_acc = trainer.evaluate(train_loader)\n",
    "print(f\"\\nüèãÔ∏è Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "preds_train, targets_train = trainer.predict_all(train_loader)\n",
    "\n",
    "print(\"\\nClassification report (Train):\")\n",
    "print(classification_report(targets_train, preds_train, digits=4))\n"
   ],
   "metadata": {
    "id": "A-FR-NYgt6Hv",
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "A-FR-NYgt6Hv",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7733709b9e83da1e"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
